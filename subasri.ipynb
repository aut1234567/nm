{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP4O4HA50m0D",
        "outputId": "3c264c5e-4247-4d1e-88d3-ce630712677f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17 unique tokens.\n",
            "Shape of data tensor: (5, 100)\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.6963 - accuracy: 0.5000 - val_loss: 0.6876 - val_accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.6895 - val_accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 184ms/step - loss: 0.6382 - accuracy: 1.0000 - val_loss: 0.6810 - val_accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.6141 - accuracy: 1.0000 - val_loss: 0.6692 - val_accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.5921 - accuracy: 1.0000 - val_loss: 0.6576 - val_accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.5702 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.5490 - accuracy: 1.0000 - val_loss: 0.6391 - val_accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 149ms/step - loss: 0.5281 - accuracy: 1.0000 - val_loss: 0.6345 - val_accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5070 - accuracy: 1.0000 - val_loss: 0.6321 - val_accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.4862 - accuracy: 1.0000 - val_loss: 0.6293 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.6293 - accuracy: 1.0000\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# Sample data\n",
        "documents = [\n",
        "    \"This is a positive review\",\n",
        "    \"Negative review, not recommended\",\n",
        "    \"Great product, highly recommended\",\n",
        "    \"Not happy with the purchase\",\n",
        "    \"Best product ever\"\n",
        "]\n",
        "labels = np.array([1, 0, 1, 0, 1])  # 1 for positive, 0 for negative\n",
        "\n",
        "# Tokenization\n",
        "max_words = 1000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(documents)\n",
        "sequences = tokenizer.texts_to_sequences(documents)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "maxlen = 100  # Maximum sequence length\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "\n",
        "# Splitting the data into train and test sets\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "training_samples = int(0.8 * len(data))\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_test = data[training_samples:]\n",
        "y_test = labels[training_samples:]\n",
        "\n",
        "# Building the CNN model\n",
        "embedding_dim = 100\n",
        "filters = 128\n",
        "kernel_size = 5\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print('Test Accuracy:', accuracy)\n"
      ]
    }
  ]
}